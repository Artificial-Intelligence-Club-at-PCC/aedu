{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Set up Gemini API (Alternative to OpenAI)\n",
    "import os\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = config[\"API_KEY\"]\n",
    "\n",
    "# Import Gemini-specific libraries\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "\n",
    "# Create a simple document store (for testing)\n",
    "documents = [\n",
    "    \"This is a sample document about RAG.\",\n",
    "    \"RAG combines retrieval and generation for better answers.\",\n",
    "    \"LangChain is a popular framework for building RAG systems.\"\n",
    "]\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store using Gemini\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Set up the RAG pipeline with Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "# Test the RAG system with a sample query\n",
    "query = \"What is RAG?\"\n",
    "result = qa_chain.run(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Implement Agentic RAG using Gemini and LangChain's AgentExecutor\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import re\n",
    "\n",
    "# Define a custom prompt template for the agent\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template = \"\"\"Answer the following question as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "    def format(self, **kwargs):\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        intermediate_steps = kwargs.pop(\"agent_scratchpad\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in kwargs[\"tools\"]])\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in kwargs[\"tools\"]])\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "# Define a custom output parser\n",
    "class CustomOutputParser:\n",
    "    def parse(self, llm_output):\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": \"I could not determine the next action.\"},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2).strip(\" \").strip('\"')\n",
    "        return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
    "\n",
    "# Define a tool for document retrieval (using the Gemini-based qa_chain)\n",
    "def retrieve_documents(query):\n",
    "    return qa_chain.run(query)\n",
    "\n",
    "tools = [Tool(name=\"RetrieveDocuments\", func=retrieve_documents, description=\"Useful for retrieving documents related to the query.\")]\n",
    "\n",
    "# Set up the agent with Gemini\n",
    "prompt = CustomPromptTemplate(template=CustomPromptTemplate.template, input_variables=[\"input\", \"agent_scratchpad\", \"tools\"])\n",
    "output_parser = CustomOutputParser()\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)  # Using the Gemini llm from the previous cell\n",
    "agent = LLMSingleActionAgent(llm_chain=llm_chain, allowed_tools=[tool.name for tool in tools], output_parser=output_parser)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Test the agentic RAG system with Gemini\n",
    "query = \"What is RAG?\"\n",
    "result = agent_executor.run(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
